"""
Script pour ex√©cuter l'√©valuation RAGAS sur le dataset de test.
"""
import sys
import json
from pathlib import Path

# Ajout du r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from src.retrieval import VectorStoreManager
from src.generation import KudoResponseGenerator
from src.evaluation import RagasEvaluator
from config import settings


def load_test_dataset(dataset_path: str) -> tuple[list[str], list[str]]:
    """
    Charge le dataset de test depuis un fichier JSON.

    Args:
        dataset_path: Chemin vers le fichier JSON

    Returns:
        Tuple (questions, ground_truths)
    """
    with open(dataset_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    return data["questions"], data["ground_truths"]


def main():
    """Fonction principale d'√©valuation."""
    logger.info("=" * 80)
    logger.info("√âVALUATION RAGAS DU SYST√àME RAG-KUDO")
    logger.info("=" * 80)

    # Chargement du dataset de test
    dataset_path = Path(__file__).parent.parent / "data" / "evaluation" / "test_dataset.json"
    logger.info(f"Chargement du dataset: {dataset_path}")

    questions, ground_truths = load_test_dataset(str(dataset_path))
    logger.info(f"‚úÖ Dataset charg√©: {len(questions)} questions")

    # Initialisation du syst√®me RAG
    logger.info("\nInitialisation du syst√®me RAG...")
    vector_manager = VectorStoreManager()
    index = vector_manager.load_index()
    generator = KudoResponseGenerator(index=index)
    logger.info("‚úÖ Syst√®me RAG initialis√©")

    # Cr√©ation de l'√©valuateur
    logger.info("\nCr√©ation de l'√©valuateur RAGAS...")
    evaluator = RagasEvaluator(generator=generator)
    logger.info("‚úÖ √âvaluateur cr√©√©")

    # Ex√©cution de l'√©valuation
    logger.info("\n" + "=" * 80)
    logger.info("D√âMARRAGE DE L'√âVALUATION")
    logger.info("=" * 80 + "\n")

    results_df = evaluator.evaluate_dataset(
        test_questions=questions,
        ground_truths=ground_truths,
    )

    # Sauvegarde des r√©sultats
    output_path = Path(__file__).parent.parent / "data" / "evaluation" / "results.csv"
    results_df.to_csv(output_path, index=False)
    logger.info(f"\nüíæ R√©sultats sauvegard√©s dans: {output_path}")

    # Affichage des r√©sultats d√©taill√©s
    logger.info("\n" + "=" * 80)
    logger.info("R√âSULTATS D√âTAILL√âS PAR QUESTION")
    logger.info("=" * 80 + "\n")

    for idx, row in results_df.iterrows():
        logger.info(f"Question {idx + 1}: {row['question'][:80]}...")
        logger.info(f"  Faithfulness:       {row.get('faithfulness', 'N/A'):.3f}")
        logger.info(f"  Answer Relevancy:   {row.get('answer_relevancy', 'N/A'):.3f}")
        logger.info(f"  Context Precision:  {row.get('context_precision', 'N/A'):.3f}")
        logger.info(f"  Context Recall:     {row.get('context_recall', 'N/A'):.3f}")
        logger.info("")

    logger.success("\n‚úÖ √âvaluation termin√©e avec succ√®s!")


if __name__ == "__main__":
    main()
