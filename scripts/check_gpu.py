#!/usr/bin/env python3
"""
Script de diagnostic GPU pour v√©rifier la configuration CUDA.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
from src.utils.gpu_utils import print_gpu_info, get_optimal_batch_size, configure_cuda_optimizations


def check_sentence_transformers():
    """V√©rifie que sentence-transformers peut utiliser CUDA."""
    print("\nüîç Test Sentence Transformers...")
    try:
        from sentence_transformers import SentenceTransformer

        model = SentenceTransformer('all-MiniLM-L6-v2')
        device = model.device

        print(f"‚úÖ Sentence Transformers charg√© sur: {device}")

        # Test d'embedding
        test_text = "Test d'embedding avec GPU"
        embedding = model.encode(test_text)
        print(f"‚úÖ Embedding g√©n√©r√©: {len(embedding)} dimensions")

    except Exception as e:
        print(f"‚ùå Erreur: {e}")


def check_docling():
    """V√©rifie la configuration Docling."""
    print("\nüîç Test Docling...")
    try:
        from docling.document_converter import DocumentConverter
        print("‚úÖ Docling import√© avec succ√®s")
        print("   Note: Docling utilisera automatiquement CUDA pour l'OCR si disponible")
    except Exception as e:
        print(f"‚ùå Erreur: {e}")


def main():
    print("\n" + "="*70)
    print("üéÆ Diagnostic GPU - RAG-Kudo")
    print("="*70)

    # Info GPU de base
    print_gpu_info()

    # Optimisations CUDA
    if torch.cuda.is_available():
        configure_cuda_optimizations()
        print("‚úÖ Optimisations CUDA activ√©es (cuDNN benchmark + TF32)")

    # Batch sizes recommand√©s
    print("\nüìä Batch sizes recommand√©s pour votre GPU:")
    print(f"  - Embeddings: {get_optimal_batch_size('embedding')}")
    print(f"  - OCR: {get_optimal_batch_size('ocr')}")

    # Tests des biblioth√®ques
    check_sentence_transformers()
    check_docling()

    # R√©sum√©
    print("\n" + "="*70)
    if torch.cuda.is_available():
        print("‚úÖ Configuration GPU: OP√âRATIONNELLE")
        print("\nüí° Conseils:")
        print("  - Vos embeddings locaux seront calcul√©s sur GPU")
        print("  - L'OCR de Docling utilisera le GPU automatiquement")
        print("  - Pour OpenAI embeddings, le GPU n'est pas utilis√© (API cloud)")
        print("\n  Ajoutez √† votre .env:")
        print("  USE_GPU=true")
        print(f"  EMBEDDING_BATCH_SIZE={get_optimal_batch_size('embedding')}")
        print(f"  OCR_BATCH_SIZE={get_optimal_batch_size('ocr')}")
    else:
        print("‚ö†Ô∏è  Pas de GPU CUDA d√©tect√© - Fonctionnement en mode CPU")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
