"""
Interface Chainlit pour le systÃ¨me RAG-Kudo.
Chat interactif avec affichage des sources et feedback.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import chainlit as cl
from loguru import logger
from llama_index.core.llms import ChatMessage, MessageRole

from src.retrieval import VectorStoreManager
from src.generation import KudoResponseGenerator
from config import settings


@cl.on_chat_start
async def start():
    """Initialisation au dÃ©marrage du chat."""
    await cl.Message(
        content="""# ğŸ¥‹ Bienvenue sur RAG-Kudo !

Je suis votre assistant pour la formation des arbitres de Kudo.

**Je peux vous aider avec :**
- ğŸ“– Les rÃ¨gles d'arbitrage en Kudo
- âš–ï¸ Le systÃ¨me de scoring et de pÃ©nalitÃ©s
- ğŸ¥Š Les techniques autorisÃ©es/interdites
- ğŸ‘” L'Ã©quipement rÃ©glementaire
- ğŸ‡«ğŸ‡· ğŸ‡¬ğŸ‡§ ğŸ‡·ğŸ‡º Questions en franÃ§ais, anglais ou russe

**Exemples de questions :**
- "Quelles sont les techniques de frappe autorisÃ©es en U16 ?"
- "Comment marque-t-on un ippon ?"
- "What is the required athlete's attire?"

Posez-moi vos questions sur l'arbitrage en Kudo ! ğŸ‘‡
""",
        author="Assistant"
    ).send()

    try:
        await cl.Message(content="â³ Chargement du systÃ¨me RAG...", author="System").send()

        vector_manager = VectorStoreManager()
        index = vector_manager.load_index()

        generator = KudoResponseGenerator(index=index)

        cl.user_session.set("vector_manager", vector_manager)
        cl.user_session.set("generator", generator)

        stats = vector_manager.get_stats()

        await cl.Message(
            content=f"""âœ… **SystÃ¨me prÃªt !**

ğŸ“Š **Statistiques :**
- Documents indexÃ©s : {stats.get('total_documents', 0)} chunks
- Collection : {stats.get('collection_name')}
- ModÃ¨le LLM : {settings.llm_model}
- Embeddings : {settings.embedding_model}
""",
            author="System"
        ).send()

    except Exception as e:
        logger.error(f"Erreur lors de l'initialisation: {e}")
        await cl.Message(
            content=f"âŒ **Erreur lors du chargement du systÃ¨me :** {e}\n\nVÃ©rifiez que l'index est crÃ©Ã© avec `python scripts/pipeline.py index`",
            author="System"
        ).send()


@cl.on_message
async def main(message: cl.Message):
    """Traitement des messages utilisateur avec streaming."""
    generator = cl.user_session.get("generator")

    if generator is None:
        await cl.Message(
            content="âŒ Le systÃ¨me RAG n'est pas initialisÃ©. Veuillez redÃ©marrer l'application.",
            author="Assistant"
        ).send()
        return

    try:
        conversation_history = cl.user_session.get("history", [])

        nodes = generator.retriever.retrieve(message.content)

        context_str = "\n\n".join([
            f"[Source {i+1}] {node.node.get_content()}"
            for i, node in enumerate(nodes)
        ])

        messages = [
            ChatMessage(
                role=MessageRole.SYSTEM,
                content="""Tu es un formateur expert en arbitrage de Kudo.

RÃˆGLES STRICTES Ã€ RESPECTER:
1. Tu dois UNIQUEMENT utiliser les informations prÃ©sentes dans le contexte fourni
2. Si une information n'est PAS dans le contexte, tu DOIS dire "Je n'ai pas cette information dans le rÃ¨glement fourni"
3. NE JAMAIS inventer, extrapoler ou ajouter des informations de ta connaissance gÃ©nÃ©rale
4. Cite EXACTEMENT les passages du rÃ¨glement sans les reformuler de maniÃ¨re substantielle
5. Si le contexte ne contient pas assez d'informations pour rÃ©pondre complÃ¨tement, indique-le clairement

Format de rÃ©ponse:
- Commence par citer la rÃ¨gle exacte du contexte
- Explique ensuite de maniÃ¨re pÃ©dagogique EN RESTANT FIDÃˆLE au texte source
- Si tu donnes un exemple, assure-toi qu'il est directement basÃ© sur le contexte fourni"""
            ),
        ]

        for msg_dict in conversation_history[-6:]:
            role = MessageRole.USER if msg_dict["role"] == "user" else MessageRole.ASSISTANT
            messages.append(ChatMessage(role=role, content=msg_dict["content"]))

        user_prompt = f"""CONTEXTE DU RÃˆGLEMENT OFFICIEL:
{context_str}

---

QUESTION DE L'UTILISATEUR:
{message.content}

---

INSTRUCTIONS:
RÃ©ponds Ã  la question en utilisant UNIQUEMENT les informations prÃ©sentes dans le contexte ci-dessus.
Si l'information n'est pas dans le contexte, indique-le clairement.
Cite les rÃ¨gles exactes du rÃ¨glement."""

        messages.append(ChatMessage(role=MessageRole.USER, content=user_prompt))

        msg = cl.Message(content="")
        await msg.send()

        response_stream = generator.llm_manager.llm.stream_chat(messages)

        for chunk in response_stream:
            if chunk.delta:
                await msg.stream_token(chunk.delta)

        await msg.update()

        conversation_history.append({"role": "user", "content": message.content})
        conversation_history.append({"role": "assistant", "content": msg.content})
        cl.user_session.set("history", conversation_history[-10:])

        confidence = generator._compute_confidence(nodes)
        num_sources = len(nodes)

        metadata_text = f"\n\n---\nğŸ“Š **Confiance:** {confidence:.1%} | ğŸ“š **Sources:** {num_sources}"
        msg.content += metadata_text
        await msg.update()

        if nodes:
            source_elements = []

            for idx, node in enumerate(nodes, 1):
                metadata = node.node.metadata
                section = metadata.get("section", "N/A")
                category = metadata.get("category", "N/A")
                article_ref = metadata.get("article_ref", "N/A")
                excerpt = node.node.get_content()[:400]
                score = node.score

                source_content = f"""**Section:** {section}
**CatÃ©gorie:** {category}
**RÃ©fÃ©rence:** {article_ref}
**Score de pertinence:** {score:.3f}

---

**Extrait du rÃ¨glement:**

{excerpt}
"""

                source_elements.append(
                    cl.Text(
                        content=source_content,
                        name=f"ğŸ“„ Source {idx}: {section}",
                        display="side"
                    )
                )

            await cl.ElementSidebar.set_title("ğŸ“š Sources consultÃ©es")
            await cl.ElementSidebar.set_elements(source_elements)

    except Exception as e:
        logger.error(f"Erreur lors de la gÃ©nÃ©ration: {e}")
        await cl.Message(
            content=f"âŒ **Erreur:** {e}\n\nVeuillez rÃ©essayer ou reformuler votre question.",
            author="Assistant"
        ).send()


@cl.on_chat_end
def end():
    """Nettoyage Ã  la fin du chat."""
    logger.info("Session de chat terminÃ©e")


@cl.on_settings_update
async def setup_settings(settings_update):
    """Mise Ã  jour des paramÃ¨tres utilisateur."""
    logger.info(f"ParamÃ¨tres mis Ã  jour: {settings_update}")


@cl.action_callback("feedback_positive")
async def on_positive_feedback(action: cl.Action):
    """Callback pour feedback positif."""
    logger.info(f"ğŸ‘ Feedback positif reÃ§u pour le message: {action.value}")

    await cl.Message(
        content="âœ… Merci pour votre retour positif ! Cela m'aide Ã  m'amÃ©liorer.",
        author="System"
    ).send()


@cl.action_callback("feedback_negative")
async def on_negative_feedback(action: cl.Action):
    """Callback pour feedback nÃ©gatif."""
    logger.info(f"ğŸ‘ Feedback nÃ©gatif reÃ§u pour le message: {action.value}")

    await cl.Message(
        content="âš ï¸ Merci pour votre retour. Pourriez-vous reformuler votre question pour que je puisse mieux vous aider ?",
        author="System"
    ).send()


if __name__ == "__main__":
    pass
