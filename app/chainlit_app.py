"""
Interface Chainlit pour le syst√®me RAG-Kudo.
Chat interactif avec affichage des sources et feedback.
"""

import sys
from pathlib import Path

# Ajout du r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

import chainlit as cl
from loguru import logger
from llama_index.core.llms import ChatMessage, MessageRole

from src.retrieval import VectorStoreManager
from src.generation import KudoResponseGenerator
from config import settings


# Variables globales
vector_manager = None
generator = None


@cl.on_chat_start
async def start():
    """Initialisation au d√©marrage du chat."""
    global vector_manager, generator

    # Message de bienvenue avec image
    await cl.Message(
        content="""# ü•ã Bienvenue sur RAG-Kudo !

Je suis votre assistant pour la formation des arbitres de Kudo.

**Je peux vous aider avec :**
- üìñ Les r√®gles d'arbitrage en Kudo
- ‚öñÔ∏è Le syst√®me de scoring et de p√©nalit√©s
- ü•ä Les techniques autoris√©es/interdites
- üëî L'√©quipement r√©glementaire
- üá´üá∑ üá¨üáß üá∑üá∫ Questions en fran√ßais, anglais ou russe

**Exemples de questions :**
- "Quelles sont les techniques de frappe autoris√©es en U16 ?"
- "Comment marque-t-on un ippon ?"
- "What is the required athlete's attire?"

Posez-moi vos questions sur l'arbitrage en Kudo ! üëá
""",
        author="Assistant"
    ).send()

    # Chargement du syst√®me RAG
    try:
        await cl.Message(content="‚è≥ Chargement du syst√®me RAG...", author="System").send()

        # Initialisation du vector store
        vector_manager = VectorStoreManager()
        index = vector_manager.load_index()

        # Initialisation du g√©n√©rateur
        generator = KudoResponseGenerator(index=index)

        # Stats
        stats = vector_manager.get_stats()

        await cl.Message(
            content=f"""‚úÖ **Syst√®me pr√™t !**

üìä **Statistiques :**
- Documents index√©s : {stats.get('total_documents', 0)} chunks
- Collection : {stats.get('collection_name')}
- Mod√®le LLM : {settings.llm_model}
- Embeddings : {settings.embedding_model}
""",
            author="System"
        ).send()

    except Exception as e:
        logger.error(f"Erreur lors de l'initialisation: {e}")
        await cl.Message(
            content=f"‚ùå **Erreur lors du chargement du syst√®me :** {e}\n\nV√©rifiez que l'index est cr√©√© avec `python scripts/pipeline.py index`",
            author="System"
        ).send()


@cl.on_message
async def main(message: cl.Message):
    """Traitement des messages utilisateur avec streaming."""
    global generator

    if generator is None:
        await cl.Message(
            content="‚ùå Le syst√®me RAG n'est pas initialis√©. Veuillez red√©marrer l'application.",
            author="Assistant"
        ).send()
        return

    try:
        # R√©cup√©ration de l'historique pour le contexte
        conversation_history = cl.user_session.get("history", [])

        # R√©cup√©ration des documents pertinents
        nodes = generator.retriever.retrieve(message.content)

        # Construction du contexte √† partir des nodes
        context_str = "\n\n".join([
            f"[Source {i+1}] {node.node.get_content()}"
            for i, node in enumerate(nodes)
        ])

        # Pr√©paration des messages pour LlamaIndex
        messages = [
            ChatMessage(
                role=MessageRole.SYSTEM,
                content="""Tu es un formateur expert en arbitrage de Kudo.

R√àGLES STRICTES √Ä RESPECTER:
1. Tu dois UNIQUEMENT utiliser les informations pr√©sentes dans le contexte fourni
2. Si une information n'est PAS dans le contexte, tu DOIS dire "Je n'ai pas cette information dans le r√®glement fourni"
3. NE JAMAIS inventer, extrapoler ou ajouter des informations de ta connaissance g√©n√©rale
4. Cite EXACTEMENT les passages du r√®glement sans les reformuler de mani√®re substantielle
5. Si le contexte ne contient pas assez d'informations pour r√©pondre compl√®tement, indique-le clairement

Format de r√©ponse:
- Commence par citer la r√®gle exacte du contexte
- Explique ensuite de mani√®re p√©dagogique EN RESTANT FID√àLE au texte source
- Si tu donnes un exemple, assure-toi qu'il est directement bas√© sur le contexte fourni"""
            ),
        ]

        # Ajout de l'historique (derniers 3 √©changes)
        for msg_dict in conversation_history[-6:]:
            role = MessageRole.USER if msg_dict["role"] == "user" else MessageRole.ASSISTANT
            messages.append(ChatMessage(role=role, content=msg_dict["content"]))

        # Ajout du contexte et de la question
        user_prompt = f"""CONTEXTE DU R√àGLEMENT OFFICIEL:
{context_str}

---

QUESTION DE L'UTILISATEUR:
{message.content}

---

INSTRUCTIONS:
R√©ponds √† la question en utilisant UNIQUEMENT les informations pr√©sentes dans le contexte ci-dessus.
Si l'information n'est pas dans le contexte, indique-le clairement.
Cite les r√®gles exactes du r√®glement."""

        messages.append(ChatMessage(role=MessageRole.USER, content=user_prompt))

        # Cr√©ation du message pour streaming
        msg = cl.Message(content="")
        await msg.send()

        # Streaming de la r√©ponse via LlamaIndex
        response_stream = generator.llm_manager.llm.stream_chat(messages)

        for chunk in response_stream:
            if chunk.delta:
                await msg.stream_token(chunk.delta)

        # Mise √† jour finale du message
        await msg.update()

        # Mise √† jour de l'historique
        conversation_history.append({"role": "user", "content": message.content})
        conversation_history.append({"role": "assistant", "content": msg.content})
        cl.user_session.set("history", conversation_history[-10:])

        # Calcul de la confiance
        confidence = generator._compute_confidence(nodes)
        num_sources = len(nodes)

        # Ajout des m√©tadonn√©es
        metadata_text = f"\n\n---\nüìä **Confiance:** {confidence:.1%} | üìö **Sources:** {num_sources}"
        msg.content += metadata_text
        await msg.update()

        # Affichage des sources dans la sidebar
        if nodes:
            source_elements = []

            for idx, node in enumerate(nodes, 1):
                metadata = node.node.metadata
                section = metadata.get("section", "N/A")
                category = metadata.get("category", "N/A")
                article_ref = metadata.get("article_ref", "N/A")
                excerpt = node.node.get_content()[:400]
                score = node.score

                # Cr√©ation du contenu format√© pour chaque source
                source_content = f"""**Section:** {section}
**Cat√©gorie:** {category}
**R√©f√©rence:** {article_ref}
**Score de pertinence:** {score:.3f}

---

**Extrait du r√®glement:**

{excerpt}
"""

                source_elements.append(
                    cl.Text(
                        content=source_content,
                        name=f"üìÑ Source {idx}: {section}",
                        display="side"
                    )
                )

            # Ouvre la sidebar avec les sources
            await cl.ElementSidebar.set_title("üìö Sources consult√©es")
            await cl.ElementSidebar.set_elements(source_elements)

    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration: {e}")
        await cl.Message(
            content=f"‚ùå **Erreur:** {e}\n\nVeuillez r√©essayer ou reformuler votre question.",
            author="Assistant"
        ).send()


@cl.on_chat_end
def end():
    """Nettoyage √† la fin du chat."""
    logger.info("Session de chat termin√©e")


# Authentification d√©sactiv√©e pour acc√®s libre
# Pour activer l'authentification, d√©commentez le code ci-dessous :
#
# @cl.password_auth_callback
# def auth_callback(username: str, password: str):
#     """Authentification simple."""
#     # Exemple : accepter n'importe quel username/password
#     return cl.User(
#         identifier=username,
#         metadata={"role": "user"}
#     )


@cl.on_settings_update
async def setup_settings(settings_update):
    """Mise √† jour des param√®tres utilisateur."""
    logger.info(f"Param√®tres mis √† jour: {settings_update}")


@cl.action_callback("feedback_positive")
async def on_positive_feedback(action: cl.Action):
    """Callback pour feedback positif."""
    logger.info(f"üëç Feedback positif re√ßu pour le message: {action.value}")

    await cl.Message(
        content="‚úÖ Merci pour votre retour positif ! Cela m'aide √† m'am√©liorer.",
        author="System"
    ).send()

    # TODO: Sauvegarder le feedback dans une base de donn√©es
    # pour analyse ult√©rieure et am√©lioration du syst√®me


@cl.action_callback("feedback_negative")
async def on_negative_feedback(action: cl.Action):
    """Callback pour feedback n√©gatif."""
    logger.info(f"üëé Feedback n√©gatif re√ßu pour le message: {action.value}")

    await cl.Message(
        content="‚ö†Ô∏è Merci pour votre retour. Pourriez-vous reformuler votre question pour que je puisse mieux vous aider ?",
        author="System"
    ).send()

    # TODO: Sauvegarder le feedback n√©gatif pour am√©lioration


if __name__ == "__main__":
    # Pour lancer : chainlit run app/chainlit_app.py
    pass
