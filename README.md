# RAG-Kudo ğŸ¥‹

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![LlamaIndex](https://img.shields.io/badge/LlamaIndex-0.12+-green.svg)](https://www.llamaindex.ai/)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-orange.svg)](https://openai.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

SystÃ¨me RAG (Retrieval-Augmented Generation) avancÃ© pour la formation des arbitres en Kudo. Utilise **LlamaIndex**, **Docling**, **RAGAS**, et **LangFuse** pour fournir des rÃ©ponses prÃ©cises et traÃ§ables basÃ©es sur le rÃ¨glement officiel.

---

## ğŸ¯ Objectif

CrÃ©er un assistant intelligent pour la formation des arbitres de Kudo qui :
- ğŸ“– RÃ©pond aux questions sur les rÃ¨gles d'arbitrage avec **fidÃ©litÃ© aux sources**
- ğŸ“š Cite les sources officielles du rÃ¨glement
- ğŸ“ Fournit des explications pÃ©dagogiques et des exemples concrets
- ğŸ“Š Mesure la qualitÃ© des rÃ©ponses avec **RAGAS**
- ğŸ” TraÃ§abilitÃ© complÃ¨te via **LangFuse**

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Documents Sources                          â”‚
â”‚              (PDF - RÃ¨glement Kudo Officiel)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Docling Processor                            â”‚
â”‚         â€¢ Extraction structurÃ©e (texte + tables)               â”‚
â”‚         â€¢ OCR pour documents scannÃ©s                           â”‚
â”‚         â€¢ DÃ©tection automatique de sections                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Semantic Chunking                             â”‚
â”‚         â€¢ Chunking intelligent avec LlamaIndex                 â”‚
â”‚         â€¢ PrÃ©servation du contexte (800 tokens, overlap 150)   â”‚
â”‚         â€¢ Enrichissement mÃ©tadonnÃ©es (sections, catÃ©gories)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Vector Store (ChromaDB)                      â”‚
â”‚         â€¢ Embeddings: text-embedding-3-small                   â”‚
â”‚         â€¢ Stockage local persistant                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Advanced Retrieval Pipeline                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Query            â”‚â†’ â”‚ Semantic Search  â”‚â†’ â”‚ Re-ranking   â”‚ â”‚
â”‚  â”‚ Reformulation    â”‚  â”‚ (Top-K = 10)     â”‚  â”‚ CrossEncoder â”‚ â”‚
â”‚  â”‚ (LLM-based)      â”‚  â”‚                  â”‚  â”‚ (Top-5)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Response Generation                           â”‚
â”‚         â€¢ LLM: GPT-4 Turbo (temperature=0.0)                   â”‚
â”‚         â€¢ Prompts optimisÃ©s pour fidÃ©litÃ© aux sources          â”‚
â”‚         â€¢ Streaming support (Chainlit UI)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Observability & Evaluation                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   LangFuse Traces   â”‚       â”‚   RAGAS Metrics     â”‚        â”‚
â”‚  â”‚  â€¢ LLM calls        â”‚       â”‚  â€¢ Faithfulness     â”‚        â”‚
â”‚  â”‚  â€¢ Retrieval logs   â”‚       â”‚  â€¢ Relevancy        â”‚        â”‚
â”‚  â”‚  â€¢ Latency tracking â”‚       â”‚  â€¢ Precision/Recall â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ CaractÃ©ristiques ClÃ©s

### ğŸ”¬ Ã‰valuation Quantitative (RAGAS)
- **Faithfulness**: 55.6% â†’ Optimisation en cours (objectif: >75%)
- **Answer Relevancy**: 86.3%
- **Context Precision**: 71.8%
- **Context Recall**: 80.0%

Voir [EVALUATION.md](EVALUATION.md) pour les dÃ©tails complets.

### ğŸ” ObservabilitÃ© (LangFuse)
- TraÃ§abilitÃ© complÃ¨te des appels LLM
- Monitoring de la latence et des coÃ»ts
- DÃ©bogage facilitÃ© des chaÃ®nes RAG

### ğŸ¯ Retrieval AvancÃ©
- **Query Reformulation**: GÃ©nÃ©ration de variations de requÃªtes avec LLM
- **Re-ranking**: CrossEncoder pour amÃ©liorer la prÃ©cision
- **Hybrid Search**: Combinaison sÃ©mantique + mÃ©tadonnÃ©es

### ğŸ’¬ Interface Interactive (Chainlit)
- Chat en temps rÃ©el avec streaming
- Affichage des sources et scores de pertinence
- Support multilingue (FR/EN/RU)

---

## ğŸ“Š RÃ©sultats d'Ã‰valuation

| MÃ©trique | Score Baseline | Statut | DÃ©tails |
|----------|---------------|--------|---------|
| **Faithfulness** | 55.6% | âš ï¸ En amÃ©lioration | LLM ajoutait 44% d'infos externes â†’ Prompts optimisÃ©s |
| **Answer Relevancy** | 86.3% | âœ… Excellent | RÃ©ponses pertinentes aux questions |
| **Context Precision** | 71.8% | âœ… Bon | Retrieval efficace |
| **Context Recall** | 80.0% | âœ… Bon | Peu d'informations manquÃ©es |

**Actions prises pour amÃ©liorer Faithfulness:**
1. Renforcement des prompts systÃ¨me (interdiction stricte d'inventer)
2. RÃ©duction tempÃ©rature: 0.1 â†’ 0.0
3. Instructions explicites de citer exactement les sources

ğŸ“ˆ [Voir l'analyse complÃ¨te](EVALUATION.md)

---

## ğŸš€ Installation

### PrÃ©requis

- Python 3.10+
- [uv](https://github.com/astral-sh/uv) (gestionnaire de packages rapide)
- ClÃ© API OpenAI (ou Anthropic)
- GPU recommandÃ© pour re-ranking (optionnel)

### Installation Rapide

```bash
# Cloner le repository
git clone https://github.com/dlakisic/RAG-Kudo.git
cd RAG-Kudo

# Installer avec uv
uv sync

# Configuration
cp .env.example .env
# Ã‰diter .env et ajouter votre OPENAI_API_KEY
```

### Configuration

Ã‰diter `.env` :

```bash
# LLM
OPENAI_API_KEY=sk-...
LLM_MODEL=gpt-4-turbo
LLM_TEMPERATURE=0.0

# Embeddings
EMBEDDING_MODEL=text-embedding-3-small

# Retrieval
TOP_K=5
USE_RERANKING=true
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# LangFuse (optionnel)
LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY=pk-...
LANGFUSE_SECRET_KEY=sk-...
```

---

## ğŸ“š Utilisation

### 1. Pipeline Complet (Ingestion â†’ Indexation â†’ Interface)

```bash
# Placer vos documents PDF dans data/raw/
cp /path/to/reglement_kudo.pdf data/raw/

# Lancer le pipeline
uv run python scripts/pipeline.py full

# Ou Ã©tape par Ã©tape:
uv run python scripts/pipeline.py ingest  # Extraction avec Docling
uv run python scripts/pipeline.py index   # Indexation vectorielle
uv run python scripts/pipeline.py query "Quelle est la valeur d'un ippon ?"
```

### 2. Interface Web (Chainlit)

```bash
# Lancer l'interface
chainlit run app/chainlit_app.py -w

# AccÃ©der Ã  http://localhost:8000
```

**FonctionnalitÃ©s de l'interface:**
- ğŸ’¬ Chat en temps rÃ©el avec streaming
- ğŸ“š Affichage des sources dans la sidebar
- ğŸ“Š Scores de confiance et de pertinence
- ğŸŒ Support FR/EN/RU

### 3. Ã‰valuation RAGAS

```bash
# Ã‰valuer le systÃ¨me sur 10 questions
uv run python scripts/run_evaluation.py

# Analyser les rÃ©sultats
uv run python scripts/analyze_results.py

# RÃ©sultats dans: data/evaluation/results.csv
```

### 4. Utilisation Programmatique

```python
from src.retrieval import VectorStoreManager
from src.generation import KudoResponseGenerator

# Charger l'index
manager = VectorStoreManager()
index = manager.load_index()

# GÃ©nÃ©rer une rÃ©ponse
generator = KudoResponseGenerator(index=index)
result = generator.generate("Quelles sont les techniques de frappe autorisÃ©es ?")

print(result["answer"])
print(f"Confiance: {result['confidence']:.1%}")
print(f"Sources: {result['num_sources']}")
```

---

## ğŸ“ Structure du Projet

```
RAG-Kudo/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Documents sources (PDF)
â”‚   â”œâ”€â”€ processed/           # Documents traitÃ©s (Docling)
â”‚   â”œâ”€â”€ vectorstore/         # ChromaDB
â”‚   â””â”€â”€ evaluation/          # RÃ©sultats RAGAS
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/           # Docling processor + chunking
â”‚   â”œâ”€â”€ retrieval/           # Vector store + retriever + re-ranker
â”‚   â”œâ”€â”€ generation/          # LLM manager + response generator
â”‚   â”œâ”€â”€ evaluation/          # RAGAS evaluator
â”‚   â”œâ”€â”€ observability/       # LangFuse integration
â”‚   â””â”€â”€ utils/               # Helpers (GPU utils, etc.)
â”œâ”€â”€ app/
â”‚   â””â”€â”€ chainlit_app.py      # Interface web Chainlit
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ pipeline.py          # Pipeline CLI principal
â”‚   â”œâ”€â”€ run_evaluation.py    # Ã‰valuation RAGAS
â”‚   â””â”€â”€ analyze_results.py   # Analyse des rÃ©sultats
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py          # Configuration Pydantic
â”œâ”€â”€ EVALUATION.md            # ğŸ“Š Rapport d'Ã©valuation dÃ©taillÃ©
â”œâ”€â”€ FEATURES.md              # Liste des fonctionnalitÃ©s
â”œâ”€â”€ QUICKSTART.md            # Guide de dÃ©marrage rapide
â””â”€â”€ README.md                # Ce fichier
```

---

## ğŸ”§ Composants Techniques

### Ingestion (Docling)
- Extraction structurÃ©e de PDFs (texte + tables)
- OCR pour documents scannÃ©s
- DÃ©tection automatique de sections et mÃ©tadonnÃ©es

### Retrieval Pipeline
1. **Query Reformulation**: LLM gÃ©nÃ¨re des variations de la question
2. **Semantic Search**: Embeddings + similaritÃ© cosinus (Top-10)
3. **Re-ranking**: CrossEncoder affine les rÃ©sultats (Top-5)

### Generation
- **Prompts optimisÃ©s** pour fidÃ©litÃ© aux sources
- **Temperature 0.0** pour rÃ©ponses dÃ©terministes
- **Citations explicites** du rÃ¨glement

### ObservabilitÃ©
- **LangFuse**: Traces LLM, latence, coÃ»ts
- **RAGAS**: Ã‰valuation quantitative (4 mÃ©triques)
- **Logs structurÃ©s** avec Loguru

---

## ğŸ“Š MÃ©triques & Performances

### Latence (sur GPU T4)
- Ingestion: ~2-3s par page PDF
- Retrieval: ~300-500ms
- Generation: ~2-4s (streaming)
- **Total**: ~3-5s par requÃªte

### CoÃ»ts (estimation)
- Embeddings: ~$0.0001 par chunk
- LLM (GPT-4 Turbo): ~$0.01-0.03 par requÃªte
- RAGAS Ã©valuation: ~$0.50-1.00 pour 10 questions

---

## ğŸ¯ Cas d'Usage

### 1. Formation d'Arbitres
- Questions/rÃ©ponses sur les rÃ¨gles
- Explications pÃ©dagogiques avec exemples
- Citations exactes du rÃ¨glement officiel

### 2. VÃ©rification de DÃ©cisions
```python
result = generator.explain_decision(
    situation="Combattant frappe aprÃ¨s l'arrÃªt",
    decision="Avertissement donnÃ©"
)
```

### 3. GÃ©nÃ©ration de Quiz
```python
quiz = generator.generate_quiz_question(category="scoring")
```

---

## ğŸ”¬ Challenges & Solutions

| Challenge | Solution ImplÃ©mentÃ©e |
|-----------|---------------------|
| **Faithfulness faible (55.6%)** | Prompts stricts + tempÃ©rature 0.0 + instructions explicites |
| **Contexte rÃ©glementaire** | Chunking sÃ©mantique + mÃ©tadonnÃ©es enrichies |
| **Multilingue (FR/EN/RU)** | Prompts adaptatifs + support Chainlit |
| **Latence retrieval** | Re-ranking sur GPU + cache |
| **TraÃ§abilitÃ©** | LangFuse pour observabilitÃ© complÃ¨te |

---

## ğŸš§ Roadmap

- [x] Pipeline d'ingestion (Docling)
- [x] Retrieval avancÃ© (reformulation + re-ranking)
- [x] Interface Chainlit
- [x] Ã‰valuation RAGAS
- [x] ObservabilitÃ© LangFuse
- [ ] Fine-tuning pour Faithfulness >80%
- [ ] API REST (FastAPI)
- [ ] Dataset Ã©tendu (50-100 questions)
- [ ] Support vidÃ©o (analyse de combats)
- [ ] DÃ©ploiement cloud (Docker + K8s)

---

## ğŸ¤ Contribution

Les contributions sont bienvenues ! Pour contribuer :

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amazing-feature`)
3. Commit (`git commit -m 'Add amazing feature'`)
4. Push (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request

---

## ğŸ“„ Licence

MIT License - voir [LICENSE](LICENSE) pour dÃ©tails.

---

## ğŸ“§ Contact

**Dino Lakisic** - [GitHub](https://github.com/dlakisic)

---

## ğŸ™ Remerciements

- [LlamaIndex](https://www.llamaindex.ai/) pour l'orchestration RAG
- [Docling](https://github.com/DS4SD/docling) pour l'extraction de documents
- [RAGAS](https://docs.ragas.io/) pour l'Ã©valuation
- [LangFuse](https://langfuse.com/) pour l'observabilitÃ©
- FÃ©dÃ©ration de Kudo pour les documents officiels

---

**Note**: Ce systÃ¨me est conÃ§u pour la formation et l'assistance. Les dÃ©cisions officielles doivent toujours Ãªtre prises en rÃ©fÃ©rence directe au rÃ¨glement officiel du Kudo.
