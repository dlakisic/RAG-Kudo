# RAG-Kudo ğŸ¥‹

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![LlamaIndex](https://img.shields.io/badge/LlamaIndex-0.12+-green.svg)](https://www.llamaindex.ai/)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-orange.svg)](https://openai.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

SystÃ¨me RAG (Retrieval-Augmented Generation) avancÃ© pour la formation des arbitres en Kudo. Utilise **LlamaIndex**, **Docling**, **RAGAS**, et **LangFuse** pour fournir des rÃ©ponses prÃ©cises et traÃ§ables basÃ©es sur le rÃ¨glement officiel.

---

## ğŸ¯ Objectif

CrÃ©er un assistant intelligent pour la formation des arbitres de Kudo qui :
- ğŸ“– RÃ©pond aux questions sur les rÃ¨gles d'arbitrage avec **fidÃ©litÃ© aux sources**
- ğŸ“š Cite les sources officielles du rÃ¨glement
- ğŸ“ Fournit des explications pÃ©dagogiques et des exemples concrets
- ğŸ“Š Mesure la qualitÃ© des rÃ©ponses avec **RAGAS**
- ğŸ” TraÃ§abilitÃ© complÃ¨te via **LangFuse**

---

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[ğŸ“„ Documents Sources<br/>PDF - RÃ¨glement Kudo Officiel] --> B[ğŸ”§ Docling Processor<br/>â€¢ Extraction structurÃ©e texte + tables<br/>â€¢ OCR pour documents scannÃ©s<br/>â€¢ DÃ©tection sections automatique]

    B --> C[âœ‚ï¸ Semantic Chunking<br/>â€¢ LlamaIndex intelligent chunking<br/>â€¢ 800 tokens, overlap 150<br/>â€¢ Enrichissement mÃ©tadonnÃ©es]

    C --> D[ğŸ’¾ Vector Store ChromaDB<br/>â€¢ text-embedding-3-small<br/>â€¢ Stockage persistant local]

    D --> E[ğŸ” Advanced Retrieval Pipeline]

    E --> E1[ğŸ”„ Query Reformulation<br/>LLM-based variations]
    E1 --> E2[ğŸ¯ Semantic Search<br/>Top-K = 10]
    E2 --> E3[âš¡ Re-ranking<br/>CrossEncoder Top-5]

    E3 --> F[ğŸ¤– Response Generation<br/>â€¢ GPT-4 Turbo temp=0.0<br/>â€¢ Prompts optimisÃ©s fidÃ©litÃ©<br/>â€¢ Streaming Chainlit]

    F --> G[ğŸ“Š Observability & Evaluation]

    G --> G1[ğŸ” LangFuse Traces<br/>â€¢ LLM calls<br/>â€¢ Retrieval logs<br/>â€¢ Latency tracking]
    G --> G2[ğŸ“ˆ RAGAS Metrics<br/>â€¢ Faithfulness<br/>â€¢ Relevancy<br/>â€¢ Precision/Recall]

    style A fill:#b3e5fc,stroke:#01579b,stroke-width:2px,color:#000
    style B fill:#ffe0b2,stroke:#e65100,stroke-width:2px,color:#000
    style C fill:#f8bbd0,stroke:#880e4f,stroke-width:2px,color:#000
    style D fill:#c8e6c9,stroke:#1b5e20,stroke-width:2px,color:#000
    style E fill:#e1bee7,stroke:#4a148c,stroke-width:2px,color:#000
    style F fill:#ffccbc,stroke:#bf360c,stroke-width:2px,color:#000
    style G fill:#cfd8dc,stroke:#263238,stroke-width:2px,color:#000
    style G1 fill:#b2dfdb,stroke:#004d40,stroke-width:2px,color:#000
    style G2 fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000
```

---

## âœ¨ CaractÃ©ristiques ClÃ©s

### ğŸ”¬ Ã‰valuation Quantitative (RAGAS)
- **Faithfulness**: 55.6% â†’ Optimisation en cours (objectif: >75%)
- **Answer Relevancy**: 86.3%
- **Context Precision**: 71.8%
- **Context Recall**: 80.0%

Voir [EVALUATION.md](EVALUATION.md) pour les dÃ©tails complets.

### ğŸ” ObservabilitÃ© (LangFuse)
- TraÃ§abilitÃ© complÃ¨te des appels LLM
- Monitoring de la latence et des coÃ»ts
- DÃ©bogage facilitÃ© des chaÃ®nes RAG

### ğŸ¯ Retrieval AvancÃ©
- **Query Reformulation**: GÃ©nÃ©ration de variations de requÃªtes avec LLM
- **Re-ranking**: CrossEncoder pour amÃ©liorer la prÃ©cision
- **Hybrid Search**: Combinaison sÃ©mantique + mÃ©tadonnÃ©es

### ğŸ’¬ Interface Interactive (Chainlit)
- Chat en temps rÃ©el avec streaming
- Affichage des sources et scores de pertinence
- Support multilingue (FR/EN/RU)

---

## ğŸ“Š RÃ©sultats d'Ã‰valuation

| MÃ©trique | Score Baseline | Statut | DÃ©tails |
|----------|---------------|--------|---------|
| **Faithfulness** | 55.6% | âš ï¸ En amÃ©lioration | LLM ajoutait 44% d'infos externes â†’ Prompts optimisÃ©s |
| **Answer Relevancy** | 86.3% | âœ… Excellent | RÃ©ponses pertinentes aux questions |
| **Context Precision** | 71.8% | âœ… Bon | Retrieval efficace |
| **Context Recall** | 80.0% | âœ… Bon | Peu d'informations manquÃ©es |

**Actions prises pour amÃ©liorer Faithfulness:**
1. Renforcement des prompts systÃ¨me (interdiction stricte d'inventer)
2. RÃ©duction tempÃ©rature: 0.1 â†’ 0.0
3. Instructions explicites de citer exactement les sources

ğŸ“ˆ [Voir l'analyse complÃ¨te](EVALUATION.md)

---

## ğŸš€ Installation

### PrÃ©requis

- Python 3.10+
- [uv](https://github.com/astral-sh/uv) (gestionnaire de packages rapide)
- ClÃ© API OpenAI (ou Anthropic)
- GPU recommandÃ© pour re-ranking (optionnel)

### Installation Rapide

```bash
# Cloner le repository
git clone https://github.com/dlakisic/RAG-Kudo.git
cd RAG-Kudo

# Installer avec uv
uv sync

# Configuration
cp .env.example .env
# Ã‰diter .env et ajouter votre OPENAI_API_KEY
```

### Configuration

Ã‰diter `.env` :

```bash
# LLM
OPENAI_API_KEY=sk-...
LLM_MODEL=gpt-4-turbo
LLM_TEMPERATURE=0.0

# Embeddings
EMBEDDING_MODEL=text-embedding-3-small

# Retrieval
TOP_K=5
USE_RERANKING=true
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# LangFuse (optionnel)
LANGFUSE_ENABLED=true
LANGFUSE_PUBLIC_KEY=pk-...
LANGFUSE_SECRET_KEY=sk-...
```

---

## ğŸ“š Utilisation

### 1. Pipeline Complet (Ingestion â†’ Indexation â†’ Interface)

```bash
# Placer vos documents PDF dans data/raw/
cp /path/to/reglement_kudo.pdf data/raw/

# Lancer le pipeline
uv run python scripts/pipeline.py full

# Ou Ã©tape par Ã©tape:
uv run python scripts/pipeline.py ingest  # Extraction avec Docling
uv run python scripts/pipeline.py index   # Indexation vectorielle
uv run python scripts/pipeline.py query "Quelle est la valeur d'un ippon ?"
```

### 2. Interface Web (Chainlit)

```bash
# Lancer l'interface
chainlit run app/chainlit_app.py -w

# AccÃ©der Ã  http://localhost:8000
```

**FonctionnalitÃ©s de l'interface:**
- ğŸ’¬ Chat en temps rÃ©el avec streaming
- ğŸ“š Affichage des sources dans la sidebar
- ğŸ“Š Scores de confiance et de pertinence
- ğŸŒ Support FR/EN/RU

### 3. Ã‰valuation RAGAS

```bash
# Ã‰valuer le systÃ¨me sur 10 questions
uv run python scripts/run_evaluation.py

# Analyser les rÃ©sultats
uv run python scripts/analyze_results.py

# RÃ©sultats dans: data/evaluation/results.csv
```

### 4. Utilisation Programmatique

```python
from src.retrieval import VectorStoreManager
from src.generation import KudoResponseGenerator

# Charger l'index
manager = VectorStoreManager()
index = manager.load_index()

# GÃ©nÃ©rer une rÃ©ponse
generator = KudoResponseGenerator(index=index)
result = generator.generate("Quelles sont les techniques de frappe autorisÃ©es ?")

print(result["answer"])
print(f"Confiance: {result['confidence']:.1%}")
print(f"Sources: {result['num_sources']}")
```

---

## ğŸ“ Structure du Projet

```
RAG-Kudo/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Documents sources (PDF)
â”‚   â”œâ”€â”€ processed/           # Documents traitÃ©s (Docling)
â”‚   â”œâ”€â”€ vectorstore/         # ChromaDB
â”‚   â””â”€â”€ evaluation/          # RÃ©sultats RAGAS
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/           # Docling processor + chunking
â”‚   â”œâ”€â”€ retrieval/           # Vector store + retriever + re-ranker
â”‚   â”œâ”€â”€ generation/          # LLM manager + response generator
â”‚   â”œâ”€â”€ evaluation/          # RAGAS evaluator
â”‚   â”œâ”€â”€ observability/       # LangFuse integration
â”‚   â””â”€â”€ utils/               # Helpers (GPU utils, etc.)
â”œâ”€â”€ app/
â”‚   â””â”€â”€ chainlit_app.py      # Interface web Chainlit
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ pipeline.py          # Pipeline CLI principal
â”‚   â”œâ”€â”€ run_evaluation.py    # Ã‰valuation RAGAS
â”‚   â””â”€â”€ analyze_results.py   # Analyse des rÃ©sultats
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py          # Configuration Pydantic
â”œâ”€â”€ EVALUATION.md            # ğŸ“Š Rapport d'Ã©valuation dÃ©taillÃ©
â”œâ”€â”€ FEATURES.md              # Liste des fonctionnalitÃ©s
â”œâ”€â”€ QUICKSTART.md            # Guide de dÃ©marrage rapide
â””â”€â”€ README.md                # Ce fichier
```

---

## ğŸ”§ Composants Techniques

### Ingestion (Docling)
- Extraction structurÃ©e de PDFs (texte + tables)
- OCR pour documents scannÃ©s
- DÃ©tection automatique de sections et mÃ©tadonnÃ©es

### Retrieval Pipeline
1. **Query Reformulation**: LLM gÃ©nÃ¨re des variations de la question
2. **Semantic Search**: Embeddings + similaritÃ© cosinus (Top-10)
3. **Re-ranking**: CrossEncoder affine les rÃ©sultats (Top-5)

### Generation
- **Prompts optimisÃ©s** pour fidÃ©litÃ© aux sources
- **Temperature 0.0** pour rÃ©ponses dÃ©terministes
- **Citations explicites** du rÃ¨glement

### ObservabilitÃ©
- **LangFuse**: Traces LLM, latence, coÃ»ts
- **RAGAS**: Ã‰valuation quantitative (4 mÃ©triques)
- **Logs structurÃ©s** avec Loguru

---

## ğŸ“Š MÃ©triques & Performances

### Latence (sur GPU T4)
- Ingestion: ~2-3s par page PDF
- Retrieval: ~300-500ms
- Generation: ~2-4s (streaming)
- **Total**: ~3-5s par requÃªte

### CoÃ»ts (estimation)
- Embeddings: ~$0.0001 par chunk
- LLM (GPT-4 Turbo): ~$0.01-0.03 par requÃªte
- RAGAS Ã©valuation: ~$0.50-1.00 pour 10 questions

---

## ğŸ¯ Cas d'Usage

### 1. Formation d'Arbitres
- Questions/rÃ©ponses sur les rÃ¨gles
- Explications pÃ©dagogiques avec exemples
- Citations exactes du rÃ¨glement officiel

### 2. VÃ©rification de DÃ©cisions
```python
result = generator.explain_decision(
    situation="Combattant frappe aprÃ¨s l'arrÃªt",
    decision="Avertissement donnÃ©"
)
```

### 3. GÃ©nÃ©ration de Quiz
```python
quiz = generator.generate_quiz_question(category="scoring")
```

---

## ğŸ”¬ Challenges & Solutions

| Challenge | Solution ImplÃ©mentÃ©e |
|-----------|---------------------|
| **Faithfulness faible (55.6%)** | Prompts stricts + tempÃ©rature 0.0 + instructions explicites |
| **Contexte rÃ©glementaire** | Chunking sÃ©mantique + mÃ©tadonnÃ©es enrichies |
| **Multilingue (FR/EN/RU)** | Prompts adaptatifs + support Chainlit |
| **Latence retrieval** | Re-ranking sur GPU + cache |
| **TraÃ§abilitÃ©** | LangFuse pour observabilitÃ© complÃ¨te |

---

## ğŸ¤ Contribution

Les contributions sont bienvenues ! Pour contribuer :

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amazing-feature`)
3. Commit (`git commit -m 'Add amazing feature'`)
4. Push (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request
